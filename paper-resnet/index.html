<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <title>ResNet | 寻梦乌托邦</title>
  <meta name="keywords" content="">
  <meta name="description" content="ResNet | 寻梦乌托邦">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="正直 勇敢 追梦的人">
<meta property="og:type" content="website">
<meta property="og:title" content="分类">
<meta property="og:url" content="http://shun-qiang.github.io/categories/index.html">
<meta property="og:site_name" content="寻梦乌托邦">
<meta property="og:description" content="正直 勇敢 追梦的人">
<meta property="og:updated_time" content="2017-12-14T07:06:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="分类">
<meta name="twitter:description" content="正直 勇敢 追梦的人">


<link rel="icon" href="/img/avatar-sun.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar-sun.jpg" />
</a>
<div class="author">
    <span>顺强</span>
</div>

<div class="icon">
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(89)</small></div></li>
    
        
            
            <li><div data-rel="深度学习">深度学习<small>(34)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="其他">其他<small>(10)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="leetcode">leetcode<small>(31)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Math">Math<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Algorithm">Algorithm<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="编程语言">编程语言<small>(5)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数据结构">数据结构<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Paper">Paper<small>(3)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    </div>
    <div><a class="about  hasFriend  site_url"  href="/about">关于</a><a style="width: 50%"  class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="89">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode === 13){return false;}">
        <input id="local-search-input" class="search" type="text" placeholder="Search..." />
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a class="color4">BFS</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Dynamic Programming</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Hard</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Binary Search</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">Graph Theory</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">quicksort</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Data Structure</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">DFS</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">distributed</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <nav id="title-list-nav">
        
        <a  class=""
           href="/about/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="关于我">关于我</span>
            <span class="post-date" title="2019-11-01 09:36:58">2019/11/01</span>
        </a>
        
        <a  class="深度学习 "
           href="/bilinear-convtranspose/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="双线性插值和转置卷积">双线性插值和转置卷积</span>
            <span class="post-date" title="2018-03-11 23:59:00">2018/03/11</span>
        </a>
        
        <a  class="其他 "
           href="/colab-offline/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Colab 防止断线">Colab 防止断线</span>
            <span class="post-date" title="2018-10-28 23:59:00">2018/10/28</span>
        </a>
        
        <a  class="深度学习 "
           href="/cv-task/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="CV Tasks">CV Tasks</span>
            <span class="post-date" title="2019-12-01 23:59:00">2019/12/01</span>
        </a>
        
        <a  class="深度学习 "
           href="/deep-learning-start/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="深度学习">深度学习</span>
            <span class="post-date" title="2019-02-18 23:59:00">2019/02/18</span>
        </a>
        
        <a  class="其他 "
           href="/git-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Git 笔记">Git 笔记</span>
            <span class="post-date" title="2015-02-16 01:20:12">2015/02/16</span>
        </a>
        
        <a  class="其他 "
           href="/hexo-troubleshoot/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Hexo troubleshooting">Hexo troubleshooting</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="深度学习 "
           href="/image-caption/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="image-caption">image-caption</span>
            <span class="post-date" title="2020-02-03 23:59:00">2020/02/03</span>
        </a>
        
        <a  class="深度学习 "
           href="/kalman-filter/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Kalman Filter">Kalman Filter</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="深度学习 "
           href="/lane-summary/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="车道线项目总结">车道线项目总结</span>
            <span class="post-date" title="2020-03-23 23:59:00">2020/03/23</span>
        </a>
        
        <a  class="其他 "
           href="/license/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="license">license</span>
            <span class="post-date" title="2018-09-15 23:59:00">2018/09/15</span>
        </a>
        
        <a  class="其他 "
           href="/linux-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Linux 笔记">Linux 笔记</span>
            <span class="post-date" title="2015-01-06 01:20:12">2015/01/06</span>
        </a>
        
        <a  class="其他 "
           href="/markdown/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MARKDOWN 小知识">MARKDOWN 小知识</span>
            <span class="post-date" title="2017-12-18 23:59:00">2017/12/18</span>
        </a>
        
        <a  class="leetcode "
           href="/maximum-number/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="585. Maximum Number in Mountain Sequence">585. Maximum Number in Mountain Sequence</span>
            <span class="post-date" title="2019-09-17 23:59:00">2019/09/17</span>
        </a>
        
        <a  class="其他 "
           href="/widows-keybord-shortcut/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="windows 快捷键">windows 快捷键</span>
            <span class="post-date" title="2015-04-21 23:59:00">2015/04/21</span>
        </a>
        
        <a  class="Math "
           href="/advanced-mathematics-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="函数的凹凸性">函数的凹凸性</span>
            <span class="post-date" title="2017-04-01 23:59:00">2017/04/01</span>
        </a>
        
        <a  class="Algorithm "
           href="/algorithm-sort-algorithm/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="排序算法">排序算法</span>
            <span class="post-date" title="2019-09-02 23:59:00">2019/09/02</span>
        </a>
        
        <a  class="编程语言 "
           href="/c-cpp/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="C++ 程序设计（面向对象进阶）">C++ 程序设计（面向对象进阶）</span>
            <span class="post-date" title="2020-02-24 23:59:00">2020/02/24</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-activate-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="激活函数">激活函数</span>
            <span class="post-date" title="2019-02-02 23:59:00">2019/02/02</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-bias-variance/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="偏差和方差（bias variance）过拟合 欠拟合（underfit overfit）">偏差和方差（bias variance）过拟合 欠拟合（underfit overfit）</span>
            <span class="post-date" title="2019-02-01 23:59:00">2019/02/01</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-bilinear-convtranspose/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="双线性插值和转置卷积">双线性插值和转置卷积</span>
            <span class="post-date" title="2018-03-11 23:59:00">2018/03/11</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-bn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Batch Normalization">Batch Normalization</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-cnn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Convolution Nerture Network">Convolution Nerture Network</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-convolution/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="卷积">卷积</span>
            <span class="post-date" title="2019-06-11 23:59:00">2019/06/11</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-data-augmentation/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="数据增强 Data Augmentation">数据增强 Data Augmentation</span>
            <span class="post-date" title="2019-12-22 23:59:00">2019/12/22</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-eval/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="评估标准">评估标准</span>
            <span class="post-date" title="2019-01-20 23:59:00">2019/01/20</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-inception/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Inception">Inception</span>
            <span class="post-date" title="2019-12-25 23:59:00">2019/12/25</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-learning-rate/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="深度学习超参数">深度学习超参数</span>
            <span class="post-date" title="2019-02-21 23:59:00">2019/02/21</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-linear-regression/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="线性回归 Linear Regression">线性回归 Linear Regression</span>
            <span class="post-date" title="2019-12-06 01:20:12">2019/12/06</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-loss-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Loss 损失函数">Loss 损失函数</span>
            <span class="post-date" title="2019-03-05 23:59:00">2019/03/05</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-mobilenet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MobileNet">MobileNet</span>
            <span class="post-date" title="2019-12-13 23:59:00">2019/12/13</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-optimization-algorithm/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="优化算法">优化算法</span>
            <span class="post-date" title="2020-03-23 23:59:00">2020/03/23</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-pooling/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="池化层">池化层</span>
            <span class="post-date" title="2019-10-12 23:59:00">2019/10/12</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-receptive-field/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="感受野">感受野</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-rethink-relu-to/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Rethinking ReLU to Train Better CNNs">Rethinking ReLU to Train Better CNNs</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-what-is-torchnn-53/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="torch.nn 到底是什么？">torch.nn 到底是什么？</span>
            <span class="post-date" title="2019-12-28 23:59:00">2019/12/28</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-xception/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Xception">Xception</span>
            <span class="post-date" title="2019-02-14 23:59:00">2019/02/14</span>
        </a>
        
        <a  class="深度学习 "
           href="/cnn-yolov2/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="YOLO9000 Better, Faster, Stronger">YOLO9000 Better, Faster, Stronger</span>
            <span class="post-date" title="2020-03-18 23:59:00">2020/03/18</span>
        </a>
        
        <a  class="数据结构 "
           href="/data-structure-tree/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Tree">Tree</span>
            <span class="post-date" title="2015-04-18 23:59:00">2015/04/18</span>
        </a>
        
        <a  class="其他 "
           href="/english-eg-word/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="英语词根词缀">英语词根词缀</span>
            <span class="post-date" title="2019-11-18 23:59:00">2019/11/18</span>
        </a>
        
        <a  class="其他 "
           href="/keybord-shortcut-chrome-ketbord-shutcut/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="chrome 快捷键">chrome 快捷键</span>
            <span class="post-date" title="2015-04-21 23:59:00">2015/04/21</span>
        </a>
        
        <a  class="其他 "
           href="/keybord-shortcut-widows-keybord-shortcut/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="windows 快捷键">windows 快捷键</span>
            <span class="post-date" title="2019-04-21 23:59:00">2019/04/21</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-102-binary-tree-level/"
           data-tag="BFS"
           data-author="" >
            <span class="post-title" title="102. Binary Tree Level Order Traversal">102. Binary Tree Level Order Traversal</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-104-leetcode/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="104. Maximum Depth of Binary Tree">104. Maximum Depth of Binary Tree</span>
            <span class="post-date" title="2019-11-18 23:59:00">2019/11/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-125-valid-palindrome/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="125. Valid Palindrome">125. Valid Palindrome</span>
            <span class="post-date" title="2019-10-16 23:59:00">2019/10/16</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-1406-stone-game-3/"
           data-tag="Dynamic Programming,Hard"
           data-author="" >
            <span class="post-title" title="1406. Stone Game III">1406. Stone Game III</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-162-find-peak-el/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="162. Find Peak Element">162. Find Peak Element</span>
            <span class="post-date" title="2020-01-13 23:59:00">2020/01/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-198-house-rob/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="198. House Robber">198. House Robber</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-200-number-of-islands/"
           data-tag="BFS,Graph Theory"
           data-author="" >
            <span class="post-title" title="200. Number of Islands">200. Number of Islands</span>
            <span class="post-date" title="2019-04-21 23:59:00">2019/04/21</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-215-kth-largest/"
           data-tag="quicksort"
           data-author="" >
            <span class="post-title" title="215. Kth Largest Element in an Array">215. Kth Largest Element in an Array</span>
            <span class="post-date" title="2020-04-10 01:20:12">2020/04/10</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-240-search-a-2D-m-2/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="240. Search a 2D Matrix II">240. Search a 2D Matrix II</span>
            <span class="post-date" title="2019-04-03 23:59:00">2019/04/03</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-28-implement-str/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="28. Implement strStr()">28. Implement strStr()</span>
            <span class="post-date" title="2019-12-27 23:59:00">2019/12/27</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-322-coin-change/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="322. Coin Change">322. Coin Change</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-35-search-insert-p/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="35. Search Insert Position">35. Search Insert Position</span>
            <span class="post-date" title="2020-03-01 23:59:00">2020/03/01</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-409-longest-palindrome/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="409. Longest Palindrome">409. Longest Palindrome</span>
            <span class="post-date" title="2017-11-01 23:59:00">2017/11/01</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-455-assign-cookies/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="455. Assign Cookies 贪心算法">455. Assign Cookies 贪心算法</span>
            <span class="post-date" title="2019-12-11 23:59:00">2019/12/11</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-5-longest-palindromic-substring/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="5. Longest Palindromic Substring">5. Longest Palindromic Substring</span>
            <span class="post-date" title="2019-11-23 23:59:00">2019/11/23</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-50-pow/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="50. Pow(x, n)">50. Pow(x, n)</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-516-longest-palindrome-subsequence/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="516. Longest Palindrome Subsequence">516. Longest Palindrome Subsequence</span>
            <span class="post-date" title="2018-02-01 23:59:00">2018/02/01</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-53-maximum-subarray/"
           data-tag="Dynamic Programming,Data Structure"
           data-author="" >
            <span class="post-title" title="53. Maximum Subarray">53. Maximum Subarray</span>
            <span class="post-date" title="2020-04-22 23:59:00">2020/04/22</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-55-jump-game/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="55. Jump Game">55. Jump Game</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-62-unique-paths/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="62. Unique Paths">62. Unique Paths</span>
            <span class="post-date" title="2020-04-16 23:59:00">2020/04/16</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-64-min-path-sun/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="64. Minimum Path Sum">64. Minimum Path Sum</span>
            <span class="post-date" title="2019-07-21 23:59:00">2019/07/21</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-685-find-k-closest-elements/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="658. Find K Closest Elements">658. Find K Closest Elements</span>
            <span class="post-date" title="2020-04-12 23:59:00">2020/04/12</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-69-combination-sum/"
           data-tag="DFS"
           data-author="" >
            <span class="post-title" title="69. Combination Sum">69. Combination Sum</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-74-search-a-2d-m/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="74. Search a 2D Matrix">74. Search a 2D Matrix</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-bfs-summary/"
           data-tag="BFS,Graph Theory"
           data-author="" >
            <span class="post-title" title="Breadth First Search Summary">Breadth First Search Summary</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-binary-search-summary/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="Binary Search 总结">Binary Search 总结</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-dfs-summary/"
           data-tag="Graph Theory,DFS"
           data-author="" >
            <span class="post-title" title="DFS Summary">DFS Summary</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-dp-summary/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="Dynamic Programming Summary">Dynamic Programming Summary</span>
            <span class="post-date" title="2019-04-18 23:59:00">2019/04/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-leetcode-exercise/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="leetcode 刷题计划">leetcode 刷题计划</span>
            <span class="post-date" title="2019-11-18 23:59:00">2019/11/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-product-nums/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Product of Array Except Self">Product of Array Except Self</span>
            <span class="post-date" title="2020-04-16 23:59:00">2020/04/16</span>
        </a>
        
        <a  class="深度学习 "
           href="/nlp-word-vector/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Efficient Estimation of Word Representations in Vector Space">Efficient Estimation of Word Representations in Vector Space</span>
            <span class="post-date" title="2019-10-21 23:59:00">2019/10/21</span>
        </a>
        
        <a  class="深度学习 "
           href="/nlp-word2vector/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="词向量">词向量</span>
            <span class="post-date" title="2020-02-18 23:59:00">2020/02/18</span>
        </a>
        
        <a  class="深度学习 "
           href="/paper-deeplab/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Deeplab">Deeplab</span>
            <span class="post-date" title="2019-12-23 23:59:00">2019/12/23</span>
        </a>
        
        <a  class="深度学习 "
           href="/paper-fcn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="FCN">FCN</span>
            <span class="post-date" title="2019-12-24 23:59:00">2019/12/24</span>
        </a>
        
        <a  class="Paper "
           href="/paper-resnet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ResNet">ResNet</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="Paper "
           href="/paper-unet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="UNET">UNET</span>
            <span class="post-date" title="2019-10-18 23:59:00">2019/10/18</span>
        </a>
        
        <a  class="Paper "
           href="/paper-vgg/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="VGG Convolutional Neural Networks">VGG Convolutional Neural Networks</span>
            <span class="post-date" title="2020-01-15 23:59:00">2020/01/15</span>
        </a>
        
        <a  class="Math "
           href="/probability-theory-digital-characteristics/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="数字特征 Digital Characteristics">数字特征 Digital Characteristics</span>
            <span class="post-date" title="2015-04-18 23:59:00">2015/04/18</span>
        </a>
        
        <a  class="Math "
           href="/probability-theory-probability-theory/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="概率论">概率论</span>
            <span class="post-date" title="2018-03-23 23:59:00">2018/03/23</span>
        </a>
        
        <a  class="深度学习 "
           href="/python-numpy-nn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Numpy 构建神经网络">Numpy 构建神经网络</span>
            <span class="post-date" title="2018-04-06 01:20:12">2018/04/06</span>
        </a>
        
        <a  class="深度学习 "
           href="/python-numpy/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Numpy">Numpy</span>
            <span class="post-date" title="2018-01-06 01:20:12">2018/01/06</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-distri/"
           data-tag="distributed"
           data-author="" >
            <span class="post-title" title="Python之分布式并行">Python之分布式并行</span>
            <span class="post-date" title="2019-09-10 23:59:00">2019/09/10</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-logging/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python logging 使用小贴士">Python logging 使用小贴士</span>
            <span class="post-date" title="2017-03-18 23:59:00">2017/03/18</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python 笔记">Python 笔记</span>
            <span class="post-date" title="2015-02-16 01:20:12">2015/02/16</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-object/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python 之面向对象">Python 之面向对象</span>
            <span class="post-date" title="2019-09-04 23:59:00">2019/09/04</span>
        </a>
        
        <a  class="深度学习 "
           href="/pytorch-pytorch-faq/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Pytorch 使用常见问题查询手册">Pytorch 使用常见问题查询手册</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="深度学习 "
           href="/pytorch-pytorch-mutil-gpu/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Pytorch 多gpu并行训练">Pytorch 多gpu并行训练</span>
            <span class="post-date" title="2019-11-14 23:59:00">2019/11/14</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-paper-resnet" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">ResNet</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a  data-rel="Paper">Paper</a>
            
        </span>
        
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2020-04-14 14:48:34'>2019-12-18 23:59</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Resnet残差网络的初衷"><span class="toc-text"><a href="#Resnet&#x6B8B;&#x5DEE;&#x7F51;&#x7EDC;&#x7684;&#x521D;&#x8877;" class="headerlink" title="Resnet&#x6B8B;&#x5DEE;&#x7F51;&#x7EDC;&#x7684;&#x521D;&#x8877;"></a>Resnet&#x6B8B;&#x5DEE;&#x7F51;&#x7EDC;&#x7684;&#x521D;&#x8877;</span></a></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Deep Residual Learning for Image Recognition<br>2015</p>
<h1 id="Resnet残差网络的初衷"><a href="#Resnet残差网络的初衷" class="headerlink" title="Resnet残差网络的初衷"></a>Resnet残差网络的初衷</h1><p>ResNet解决传统深度神经网络造成的问题:<br>梯度消失<br>网络性能退化<br>可以认为Residual Learning的初衷，其实是让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化！<br>每一层卷积都是将底层的特征提取出来组合成高级特征，所以网络的层数越多，后面的特征描述的东西越抽象。</p>
<p>首先，浅层网络都是希望学习到一个恒等映射函数 $H(X) = X$ ,其中 = 是指用 $H(x)$ 这个特征/函数来代表原始 $x$ 的信息，但随着网络的加深这个恒等映射变得越来越难以拟合。<br>因此，Resnet 提出将网络设计为 $F(x) = H(x) - x$，只要残差 $F(x) = 0$ ，就构成了一个恒等映射 $H(x) = x$ ，并且相对于拟合恒等映射关系，拟合残差更容易。 </p>
<p>从浅层到深层学习到的特征 $y = x + F(x,W)$，其中 F(x,W) 就是带权卷积后的结果，我们进行反向求梯度：<br>$\frac {dloss}{dx} = \frac {dloss}{dy} * \frac {dy}{dx} = \frac {dloss}{dy} * (1 + \frac {dF(x,W)}{dx})$</p>
<p>Think of any arbitrary task such as playing basketball. You might not be the best out of your friends, but you know what the best look like after watching the Golden State Warriors play all season. So the question is, how do you get from where you are to the level Stephen Curry is playing at? There are two ways of looking at your progress: “how do I be as good as Steph Curry?” and “what steps do I need to take to be as good as Steph?”. Which do you think is the better question to ask?</p>
<p>These two questions are asking the exact same question, but this is the same thought process that led to the key findings behind ResNets. The first question is comparable to a traditional network where you’re given a starting point and need to find a way to get to your goal or F(x)=H(x)-x. ResNet asks the second question. Given a starting point, how do you take constantly improve, taking into account your past state and your improvement at every step or H(x)=F(x)+x.</p>
<p>Back to the degradation problem. Let’s say x is equal to the optimal solution. In a ‘plain’ network, F(x) would still need to do the work of creating a non-linear function that maps to x. However ResNets utilize past features in it’s forward propagation. Therefore in this particular scenario, F(x) would optimize to 0 if x is already the solution. While this scenario isn’t very likely, it highlights the cause of degradation in deep networks, as well as residual learning’s approach to solving it.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Resnet18 和 Resnet101 在实现时候有个最大的不同是哪里？Resnet小于50layers和大于50layers结构区别在哪里？</span><br><span class="line">50层以下叫 basic block 3*3，3*3，</span><br><span class="line">50层以上叫 Bottleneck block 1*1，3*3，1*1，channel 64 64 256 减少参数</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""Untitled16.ipynb</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Automatically generated by Colaboratory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original file is located at</span></span><br><span class="line"><span class="string">    https://colab.research.google.com/drive/1PBNv8qCKce_i_BG_kFjDUSuiEqNtcY1U</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># import一下nn这个模块， 该模块已经封装了定义ResNet所需要的所有函数</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.hub <span class="keyword">import</span> load_state_dict_from_url</span><br><span class="line"></span><br><span class="line"><span class="comment">## 已经训练好的模型权重值</span></span><br><span class="line">model_urls = &#123;</span><br><span class="line">    <span class="string">'resnet18'</span>: <span class="string">'https://download.pytorch.org/models/resnet18-5c106cde.pth'</span>,</span><br><span class="line">    <span class="string">'resnet34'</span>: <span class="string">'https://download.pytorch.org/models/resnet34-333f7ec4.pth'</span>,</span><br><span class="line">    <span class="string">'resnet50'</span>: <span class="string">'https://download.pytorch.org/models/resnet50-19c8e357.pth'</span>,</span><br><span class="line">    <span class="string">'resnet101'</span>: <span class="string">'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'</span>,</span><br><span class="line">    <span class="string">'resnet152'</span>: <span class="string">'https://download.pytorch.org/models/resnet152-b121ed2d.pth'</span>,</span><br><span class="line">    <span class="string">'resnext50_32x4d'</span>: <span class="string">'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth'</span>,</span><br><span class="line">    <span class="string">'resnext101_32x8d'</span>: <span class="string">'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth'</span>,</span><br><span class="line">    <span class="string">'wide_resnet50_2'</span>: <span class="string">'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth'</span>,</span><br><span class="line">    <span class="string">'wide_resnet101_2'</span>: <span class="string">'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一下conv3x3，残差模块中都有至少一个3x3 的卷积</span></span><br><span class="line"><span class="comment">#return的地方直接返回一个Conv2d的输出， 卷积核预设为3， padding值为1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv3x3</span><span class="params">(in_planes, out_planes, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">3</span>, stride=stride, padding=padding, bias=<span class="keyword">False</span>)  <span class="comment">#? Why no bias:如果卷积层之后是BN层，那么可以不用偏置参数,被激活函数前的BatchNorm层的 β 给取代了，可以节省内存,具体原因请看论文 Section 3.2 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一下conv1x1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv1x1</span><span class="params">(in_planes, out_planes, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>) <span class="comment">#? Why no bias: 如果卷积层之后是BN层，那么可以不用偏置参数，可以节省内存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个类继承nn.Module模块</span></span><br><span class="line"><span class="comment"># 类的初始化中， 定义所有会用到的属性(conv, bn, relu)</span></span><br><span class="line"><span class="comment"># 定义forward function建立数据输入到return的过程</span></span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  expansion = <span class="number">1</span> <span class="comment"># 经过Block之后channel的变化量,主要是定义输出通道的放大倍率， 在bottleneck会用上</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None, norm_layer=None)</span>:</span></span><br><span class="line">    <span class="comment"># downsample: 调整维度一致之后才能相加</span></span><br><span class="line">    <span class="comment"># norm_layer：batch normalization layer</span></span><br><span class="line">true<span class="comment">#记得继承父类</span></span><br><span class="line">    super(BasicBlock, self).__init__()</span><br><span class="line">    <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      norm_layer = nn.BatchNorm2d <span class="comment"># 如果bn层没有自定义，就使用标准的bn层</span></span><br><span class="line">    self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">true<span class="comment">#BN通常依据上一层输出的维度做BN</span></span><br><span class="line">    self.bn1 = norm_layer(planes)</span><br><span class="line">true<span class="comment">#inplace表示对原数据修改,而非产生新数据,节省内存</span></span><br><span class="line">    self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">    self.conv2 = conv3x3(planes, planes)</span><br><span class="line">    self.bn2 = norm_layer(planes)</span><br><span class="line">    self.downsample = downsample</span><br><span class="line">    self.stride = stride</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    identity = x  <span class="comment"># 保存x</span></span><br><span class="line"></span><br><span class="line">    out = self.conv1(x)</span><br><span class="line">    out = self.bn1(out)</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    out = self.conv2(out)</span><br><span class="line">    out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">      identity = self.downsample(x)  <span class="comment"># downsample调整x的维度，F(x)+x一致才能相加</span></span><br><span class="line">    </span><br><span class="line">    out += identity</span><br><span class="line">    out = self.relu(out) <span class="comment"># 先相加再激活</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 残差模块Bottleneck， 加入了conv1x1 减少了参数量， 主要给网络层数较深的使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="comment">#expansion = 4 ：bottleneck的最后一层1x1输出的维度是第1(conv1x1), 2(conv3x3)层的四倍, 因此放大倍率为4</span></span><br><span class="line">  expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None, norm_layer=None)</span>:</span></span><br><span class="line">    super(Bottleneck, self).__init__()</span><br><span class="line">    <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      norm_layer = nn.BatchNorm2d</span><br><span class="line">    <span class="comment"># 主结构变成 1x1, 3x3, 1x1</span></span><br><span class="line">    self.conv1 = conv1x1(inplanes, planes)</span><br><span class="line">    self.bn1 = norm_layer(planes)</span><br><span class="line">    self.conv2 = conv3x3(planes, planes, stride)</span><br><span class="line">    self.bn2 = norm_layer(planes)</span><br><span class="line">    self.conv3 = conv1x1(planes, planes * self.expansion) <span class="comment"># 输入的channel数：planes * self.expansion</span></span><br><span class="line">    self.bn3 = norm_layer(planes * self.expansion)</span><br><span class="line">    self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">    self.downsample = downsample</span><br><span class="line">    self.stride = stride</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    identity = x</span><br><span class="line"></span><br><span class="line">    out = self.conv1(x)</span><br><span class="line">    out = self.bn1(out)</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    out = self.conv2(out)</span><br><span class="line">    out = self.bn2(out)</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    out = self.conv3(out)</span><br><span class="line">    out = self.bn3(out)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">      identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">    out += identity</span><br><span class="line">    out = self.relu(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, layers, num_class=<span class="number">1000</span>, norm_layer=None)</span>:</span></span><br><span class="line">    super(ResNet, self).__init__()</span><br><span class="line">    <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      norm_layer = nn.BatchNorm2d</span><br><span class="line">    self._norm_layer = norm_layer</span><br><span class="line">true<span class="comment"># 第一个stage通道数一定是64, 因为先经过(64, 7, 7)的conv1</span></span><br><span class="line">    self.inplanes = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv1 in ppt figure</span></span><br><span class="line">    self.conv1 = nn.Conv2d(<span class="number">3</span>, self.inplanes, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">    self.bn1 = norm_layer(self.inplanes)</span><br><span class="line">    self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">    self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    self.layer1 = self._make_layer(block, <span class="number">64</span>, layers[<span class="number">0</span>])</span><br><span class="line">    self.layer2 = self._make_layer(block, <span class="number">128</span>, layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">    self.layer3 = self._make_layer(block, <span class="number">256</span>, layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">    self.layer4 = self._make_layer(block, <span class="number">512</span>, layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">true</span><br><span class="line">true<span class="comment"># 全局平均池化</span></span><br><span class="line">    self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))  <span class="comment"># (1,1)等于GAP</span></span><br><span class="line">    self.fc = nn.Linear(<span class="number">512</span>*block.expansion, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">truetruetrue<span class="comment">#只要是卷积都操作， 都对weight和bias进行kaiming初始化</span></span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line">truetruetrue<span class="comment">#bn层都权重初始化为1， bias=0</span></span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># ResNet 的 stage 2~5实现，然后利用列表将每一个stage的blocks数目装进</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, block, planes, blocks, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 生成不同的stage/layer</span></span><br><span class="line">    <span class="comment"># block: block type(basic block/bottle block)</span></span><br><span class="line">    <span class="comment"># blocks: blocks的数量</span></span><br><span class="line">    norm_layer = self._norm_layer</span><br><span class="line">    downsample = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">      <span class="comment"># 需要调整维度</span></span><br><span class="line">      downsample = nn.Sequential(</span><br><span class="line">          conv1x1(self.inplanes, planes * block.expansion, stride),  <span class="comment"># 同时调整spatial(H x W))和channel两个方向</span></span><br><span class="line">          norm_layer(planes * block.expansion)</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    layers = []</span><br><span class="line">    layers.append(block(self.inplanes, planes, stride, downsample, norm_layer)) <span class="comment"># 第一个block单独处理</span></span><br><span class="line">true<span class="comment"># 确保上一层输出与下一层的输入通道数相同</span></span><br><span class="line">    self.inplanes = planes * block.expansion  <span class="comment"># 记录layerN的channel变化，具体请看ppt resnet表格</span></span><br><span class="line">true<span class="comment"># blocks(设定每stage多少blocks), 有几个block就添加blocks-1个（前面已经添加第一个block）</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, blocks): <span class="comment"># 从1开始循环，因为第一个模块前面已经单独处理</span></span><br><span class="line">      layers.append(block(self.inplanes, planes, norm_layer=norm_layer))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)  <span class="comment"># 使用Sequential层组合blocks，形成stage。如果layers=[2,3,4]，那么*layers=？9层</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">    x = self.layer1(x)</span><br><span class="line">    x = self.layer2(x)</span><br><span class="line">    x = self.layer3(x)</span><br><span class="line">    x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">    x = self.avgpool(x)</span><br><span class="line">    x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">    x = self.fc(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_resnet</span><span class="params">(arch, block, layers, pretrained, progress, **kwargs)</span>:</span></span><br><span class="line">    model = ResNet(block, layers, **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(model_urls[arch],</span><br><span class="line">                                              progress=progress)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用ResNet这个类， 并且指定需要传入的block类别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet18</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-18 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet18'</span>, BasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet50</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-50 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet50'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line">model = resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model.eval()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model2 = torch.hub.load(<span class="string">'pytorch/vision:v0.4.2'</span>, <span class="string">'resnet18'</span>, pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># or any of these variants</span></span><br><span class="line"><span class="comment"># model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet34', pretrained=True)</span></span><br><span class="line"><span class="comment"># model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet50', pretrained=True)</span></span><br><span class="line"><span class="comment"># model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet101', pretrained=True)</span></span><br><span class="line"><span class="comment"># model = torch.hub.load('pytorch/vision:v0.4.2', 'resnet152', pretrained=True)</span></span><br><span class="line">model2.eval()</span><br><span class="line"></span><br><span class="line">model.state_dict()</span><br><span class="line"></span><br><span class="line">model2.state_dict()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download an example image from the pytorch website</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">url, filename = (<span class="string">"https://github.com/pytorch/hub/raw/master/dog.jpg"</span>, <span class="string">"dog.jpg"</span>)</span><br><span class="line"><span class="keyword">try</span>: urllib.URLopener().retrieve(url, filename)</span><br><span class="line"><span class="keyword">except</span>: urllib.request.urlretrieve(url, filename)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sample execution (requires torchvision)</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">input_image = Image.open(filename)</span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">])</span><br><span class="line">input_tensor = preprocess(input_image)</span><br><span class="line">input_batch = input_tensor.unsqueeze(<span class="number">0</span>) <span class="comment"># create a mini-batch as expected by the model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># move the input and model to GPU for speed if available</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    input_batch = input_batch.to(<span class="string">'cuda'</span>)</span><br><span class="line">    model.to(<span class="string">'cuda'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(input_batch)</span><br><span class="line"><span class="comment"># Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes</span></span><br><span class="line">print(output[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># The output has unnormalized scores. To get probabilities, you can run a softmax on it.</span></span><br><span class="line">print(torch.nn.functional.softmax(output[<span class="number">0</span>], dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">result = torch.nn.functional.softmax(output[<span class="number">0</span>], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">result.argmax()</span><br><span class="line"></span><br><span class="line">input_image</span><br></pre></td></tr></table></figure>
<p>参考链接：<br><a href="https://mc.ai/understanding-resnet-intuitively/" target="_blank" rel="noopener">https://mc.ai/understanding-resnet-intuitively/</a></p>

      
       <hr><span style="font-style: italic;color: gray;"> 请多多指教。 </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>ResNet</p>
    
    <p><span class="copy-title">本文作者:</span><a  title="顺强">顺强</a></p>
    <p><span class="copy-title">发布时间:</span>2019-12-18, 23:59:00</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/paper-resnet/" title="ResNet">http://shun-qiang.github.io/paper-resnet/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: '84f77e333f2e54aee8e3',
            clientSecret: '9b89a9d1c0f55413c892dceedee0274ce40f34cf',
            repo: 'shun-qiang.github.io',
            owner: 'Shun-Qiang',
            admin: ['Shun-Qiang'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2020 SHUNQIANG</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>

<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#BFS','#Dynamic Programming','#Hard','#Binary Search','#Graph Theory','#quicksort','#Data Structure','#DFS','#distributed',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
        /* 渲染*/
        function HTMLDecode(text) {
            var temp = document.createElement("div");
            temp.innerHTML = text;
            var output = temp.innerText || temp.textContent;
            temp = null;
            return output;
        }
        if (window.mermaid){
            window.mermaid = null
        }
        $.getScript("//cdn.jsdelivr.net/npm/mermaid@8.4.2/dist/mermaid.min.js", function () {
            var mermaidOptions = JSON.parse(HTMLDecode("{&#34;theme&#34;:&#34;default&#34;,&#34;startOnLoad&#34;:true,&#34;flowchart&#34;:{&#34;useMaxWidth&#34;:true,&#34;htmlLabels&#34;:true}}"))
            if (window.mermaid) {
                mermaid.initialize(mermaidOptions)
                mermaid.contentLoaded()
            }
        })
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
