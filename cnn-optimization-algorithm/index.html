<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <title>优化算法 | 寻梦乌托邦</title>
  <meta name="keywords" content="">
  <meta name="description" content="优化算法 | 寻梦乌托邦">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="正直 勇敢 追梦的人">
<meta property="og:type" content="website">
<meta property="og:title" content="分类">
<meta property="og:url" content="http://shun-qiang.github.io/categories/index.html">
<meta property="og:site_name" content="寻梦乌托邦">
<meta property="og:description" content="正直 勇敢 追梦的人">
<meta property="og:updated_time" content="2017-12-14T07:06:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="分类">
<meta name="twitter:description" content="正直 勇敢 追梦的人">


<link rel="icon" href="/img/avatar-sun.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar-sun.jpg" />
</a>
<div class="author">
    <span>顺强</span>
</div>

<div class="icon">
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(89)</small></div></li>
    
        
            
            <li><div data-rel="CNN">CNN<small>(27)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="其他">其他<small>(10)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="CV">CV<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="leetcode">leetcode<small>(31)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Math">Math<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="algorithm">algorithm<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="编程语言">编程语言<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="NUMPY">NUMPY<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Pytorch">Pytorch<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数据结构">数据结构<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="NLP">NLP<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="Paper">Paper<small>(3)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    </div>
    <div><a class="about  hasFriend  site_url"  href="/about">关于</a><a style="width: 50%"  class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="89">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode === 13){return false;}">
        <input id="local-search-input" class="search" type="text" placeholder="Search..." />
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a class="color4">BFS</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Dynamic Programming</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Hard</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">Binary Search</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color3">Graph Theory</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">quicksort</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color5">Data Structure</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color4">DFS</a>
    </li>
    
    <li class="article-tag-list-item">
        <a class="color2">distributed</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <nav id="title-list-nav">
        
        <a  class=""
           href="/about/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="关于我">关于我</span>
            <span class="post-date" title="2019-11-01 09:36:58">2019/11/01</span>
        </a>
        
        <a  class="CNN "
           href="/bilinear-convtranspose/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="双线性插值和转置卷积">双线性插值和转置卷积</span>
            <span class="post-date" title="2018-03-11 23:59:00">2018/03/11</span>
        </a>
        
        <a  class="其他 "
           href="/colab-offline/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Colab 防止断线">Colab 防止断线</span>
            <span class="post-date" title="2018-10-28 23:59:00">2018/10/28</span>
        </a>
        
        <a  class="CV "
           href="/cv-task/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="CV Tasks">CV Tasks</span>
            <span class="post-date" title="2019-12-01 23:59:00">2019/12/01</span>
        </a>
        
        <a  class="CNN "
           href="/deep-learning-start/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="深度学习">深度学习</span>
            <span class="post-date" title="2019-02-18 23:59:00">2019/02/18</span>
        </a>
        
        <a  class="其他 "
           href="/git-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Git 笔记">Git 笔记</span>
            <span class="post-date" title="2015-02-16 01:20:12">2015/02/16</span>
        </a>
        
        <a  class="其他 "
           href="/hexo-troubleshoot/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Hexo troubleshooting">Hexo troubleshooting</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/image-caption/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="image-caption">image-caption</span>
            <span class="post-date" title="2020-02-03 23:59:00">2020/02/03</span>
        </a>
        
        <a  class="CNN "
           href="/kalman-filter/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Kalman Filter">Kalman Filter</span>
            <span class="post-date" title="2015-04-18 23:59:00">2015/04/18</span>
        </a>
        
        <a  class="CV "
           href="/lane-summary/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="车道线项目总结">车道线项目总结</span>
            <span class="post-date" title="2020-03-23 23:59:00">2020/03/23</span>
        </a>
        
        <a  class="其他 "
           href="/license/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="license">license</span>
            <span class="post-date" title="2018-09-15 23:59:00">2018/09/15</span>
        </a>
        
        <a  class="其他 "
           href="/linux-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Linux 笔记">Linux 笔记</span>
            <span class="post-date" title="2015-01-06 01:20:12">2015/01/06</span>
        </a>
        
        <a  class="其他 "
           href="/markdown/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MARKDOWN 小知识">MARKDOWN 小知识</span>
            <span class="post-date" title="2017-12-18 23:59:00">2017/12/18</span>
        </a>
        
        <a  class="leetcode "
           href="/maximum-number/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="585. Maximum Number in Mountain Sequence">585. Maximum Number in Mountain Sequence</span>
            <span class="post-date" title="2019-09-17 23:59:00">2019/09/17</span>
        </a>
        
        <a  class="其他 "
           href="/widows-keybord-shortcut/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="windows 快捷键">windows 快捷键</span>
            <span class="post-date" title="2015-04-21 23:59:00">2015/04/21</span>
        </a>
        
        <a  class="Math "
           href="/advanced-mathematics-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="函数的凹凸性">函数的凹凸性</span>
            <span class="post-date" title="2017-04-01 23:59:00">2017/04/01</span>
        </a>
        
        <a  class="algorithm "
           href="/algorithm-sort-algorithm/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="排序算法">排序算法</span>
            <span class="post-date" title="2019-09-02 23:59:00">2019/09/02</span>
        </a>
        
        <a  class="编程语言 "
           href="/c-cpp/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="C++ 程序设计（面向对象进阶）">C++ 程序设计（面向对象进阶）</span>
            <span class="post-date" title="2020-02-24 23:59:00">2020/02/24</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-bias-variance/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="偏差和方差（bias variance）过拟合 欠拟合（underfit overfit）">偏差和方差（bias variance）过拟合 欠拟合（underfit overfit）</span>
            <span class="post-date" title="2019-02-01 23:59:00">2019/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-activate-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="激活函数">激活函数</span>
            <span class="post-date" title="2019-02-02 23:59:00">2019/02/02</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-bilinear-convtranspose/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="双线性插值和转置卷积">双线性插值和转置卷积</span>
            <span class="post-date" title="2018-03-11 23:59:00">2018/03/11</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-bn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Batch Normalization">Batch Normalization</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-cnn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Convolution Nerture Network">Convolution Nerture Network</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-convolution/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="卷积">卷积</span>
            <span class="post-date" title="2019-06-11 23:59:00">2019/06/11</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-data-augmentation/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="数据增强 Data Augmentation">数据增强 Data Augmentation</span>
            <span class="post-date" title="2019-12-22 23:59:00">2019/12/22</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-eval/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="评估标准">评估标准</span>
            <span class="post-date" title="2019-01-20 23:59:00">2019/01/20</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-inception/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Inception">Inception</span>
            <span class="post-date" title="2019-12-25 23:59:00">2019/12/25</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-learning-rate/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="深度学习超参数">深度学习超参数</span>
            <span class="post-date" title="2019-02-21 23:59:00">2019/02/21</span>
        </a>
        
        <a  class="NUMPY "
           href="/cnn-linear-regression/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="线性回归 Linear Regression">线性回归 Linear Regression</span>
            <span class="post-date" title="2019-12-06 01:20:12">2019/12/06</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-loss-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Loss 损失函数">Loss 损失函数</span>
            <span class="post-date" title="2019-03-05 23:59:00">2019/03/05</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-mobilenet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MobileNet">MobileNet</span>
            <span class="post-date" title="2019-12-13 23:59:00">2019/12/13</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-optimization-algorithm/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="优化算法">优化算法</span>
            <span class="post-date" title="2020-03-23 23:59:00">2020/03/23</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-pooling/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="池化层">池化层</span>
            <span class="post-date" title="2019-10-12 23:59:00">2019/10/12</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-receptive-field/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="感受野">感受野</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-rethink-relu-to/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Rethinking ReLU to Train Better CNNs">Rethinking ReLU to Train Better CNNs</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-xception/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Xception">Xception</span>
            <span class="post-date" title="2019-02-14 23:59:00">2019/02/14</span>
        </a>
        
        <a  class="Pytorch "
           href="/cnn-what-is-torchnn-53/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="torch.nn 到底是什么？">torch.nn 到底是什么？</span>
            <span class="post-date" title="2019-12-28 23:59:00">2019/12/28</span>
        </a>
        
        <a  class="CNN "
           href="/cnn-yolov2/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="YOLO9000 Better, Faster, Stronger">YOLO9000 Better, Faster, Stronger</span>
            <span class="post-date" title="2020-03-18 23:59:00">2020/03/18</span>
        </a>
        
        <a  class="数据结构 "
           href="/data-structure-tree/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Tree">Tree</span>
            <span class="post-date" title="2015-04-18 23:59:00">2015/04/18</span>
        </a>
        
        <a  class="其他 "
           href="/english-eg-word/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="英语词根词缀">英语词根词缀</span>
            <span class="post-date" title="2019-11-18 23:59:00">2019/11/18</span>
        </a>
        
        <a  class="其他 "
           href="/keybord-shortcut-chrome-ketbord-shutcut/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="chrome 快捷键">chrome 快捷键</span>
            <span class="post-date" title="2015-04-21 23:59:00">2015/04/21</span>
        </a>
        
        <a  class="其他 "
           href="/keybord-shortcut-widows-keybord-shortcut/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="windows 快捷键">windows 快捷键</span>
            <span class="post-date" title="2019-04-21 23:59:00">2019/04/21</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-102-binary-tree-level/"
           data-tag="BFS"
           data-author="" >
            <span class="post-title" title="102. Binary Tree Level Order Traversal">102. Binary Tree Level Order Traversal</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-104-leetcode/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="104. Maximum Depth of Binary Tree">104. Maximum Depth of Binary Tree</span>
            <span class="post-date" title="2019-11-18 23:59:00">2019/11/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-125-valid-palindrome/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="125. Valid Palindrome">125. Valid Palindrome</span>
            <span class="post-date" title="2019-10-16 23:59:00">2019/10/16</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-1406-stone-game-3/"
           data-tag="Dynamic Programming,Hard"
           data-author="" >
            <span class="post-title" title="1406. Stone Game III">1406. Stone Game III</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-162-find-peak-el/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="162. Find Peak Element">162. Find Peak Element</span>
            <span class="post-date" title="2020-01-13 23:59:00">2020/01/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-198-house-rob/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="198. House Robber">198. House Robber</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-200-number-of-islands/"
           data-tag="BFS,Graph Theory"
           data-author="" >
            <span class="post-title" title="200. Number of Islands">200. Number of Islands</span>
            <span class="post-date" title="2019-04-21 23:59:00">2019/04/21</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-215-kth-largest/"
           data-tag="quicksort"
           data-author="" >
            <span class="post-title" title="215. Kth Largest Element in an Array">215. Kth Largest Element in an Array</span>
            <span class="post-date" title="2020-04-10 01:20:12">2020/04/10</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-240-search-a-2D-m-2/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="240. Search a 2D Matrix II">240. Search a 2D Matrix II</span>
            <span class="post-date" title="2019-04-03 23:59:00">2019/04/03</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-28-implement-str/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="28. Implement strStr()">28. Implement strStr()</span>
            <span class="post-date" title="2019-12-27 23:59:00">2019/12/27</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-35-search-insert-p/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="35. Search Insert Position">35. Search Insert Position</span>
            <span class="post-date" title="2020-03-01 23:59:00">2020/03/01</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-322-coin-change/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="322. Coin Change">322. Coin Change</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-455-assign-cookies/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="455. Assign Cookies 贪心算法">455. Assign Cookies 贪心算法</span>
            <span class="post-date" title="2019-12-11 23:59:00">2019/12/11</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-409-longest-palindrome/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="409. Longest Palindrome">409. Longest Palindrome</span>
            <span class="post-date" title="2017-11-01 23:59:00">2017/11/01</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-5-longest-palindromic-substring/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="5. Longest Palindromic Substring">5. Longest Palindromic Substring</span>
            <span class="post-date" title="2019-11-23 23:59:00">2019/11/23</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-516-longest-palindrome-subsequence/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="516. Longest Palindrome Subsequence">516. Longest Palindrome Subsequence</span>
            <span class="post-date" title="2018-02-01 23:59:00">2018/02/01</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-50-pow/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="50. Pow(x, n)">50. Pow(x, n)</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-53-maximum-subarray/"
           data-tag="Dynamic Programming,Data Structure"
           data-author="" >
            <span class="post-title" title="53. Maximum Subarray">53. Maximum Subarray</span>
            <span class="post-date" title="2020-04-22 23:59:00">2020/04/22</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-55-jump-game/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="55. Jump Game">55. Jump Game</span>
            <span class="post-date" title="2019-08-18 23:59:00">2019/08/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-62-unique-paths/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="62. Unique Paths">62. Unique Paths</span>
            <span class="post-date" title="2020-04-16 23:59:00">2020/04/16</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-64-min-path-sun/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="64. Minimum Path Sum">64. Minimum Path Sum</span>
            <span class="post-date" title="2019-07-21 23:59:00">2019/07/21</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-685-find-k-closest-elements/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="658. Find K Closest Elements">658. Find K Closest Elements</span>
            <span class="post-date" title="2020-04-12 23:59:00">2020/04/12</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-69-combination-sum/"
           data-tag="DFS"
           data-author="" >
            <span class="post-title" title="69. Combination Sum">69. Combination Sum</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-74-search-a-2d-m/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="74. Search a 2D Matrix">74. Search a 2D Matrix</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-bfs-summary/"
           data-tag="BFS,Graph Theory"
           data-author="" >
            <span class="post-title" title="Breadth First Search Summary">Breadth First Search Summary</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-binary-search-summary/"
           data-tag="Binary Search"
           data-author="" >
            <span class="post-title" title="Binary Search 总结">Binary Search 总结</span>
            <span class="post-date" title="2020-04-13 23:59:00">2020/04/13</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-dfs-summary/"
           data-tag="Graph Theory,DFS"
           data-author="" >
            <span class="post-title" title="DFS Summary">DFS Summary</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-dp-summary/"
           data-tag="Dynamic Programming"
           data-author="" >
            <span class="post-title" title="Dynamic Programming Summary">Dynamic Programming Summary</span>
            <span class="post-date" title="2019-04-18 23:59:00">2019/04/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-leetcode-exercise/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="leetcode 刷题计划">leetcode 刷题计划</span>
            <span class="post-date" title="2019-11-18 23:59:00">2019/11/18</span>
        </a>
        
        <a  class="leetcode "
           href="/leetcode-product-nums/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Product of Array Except Self">Product of Array Except Self</span>
            <span class="post-date" title="2020-04-16 23:59:00">2020/04/16</span>
        </a>
        
        <a  class="NLP "
           href="/nlp-word2vector/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="词向量">词向量</span>
            <span class="post-date" title="2020-02-18 23:59:00">2020/02/18</span>
        </a>
        
        <a  class="NLP "
           href="/nlp-word-vector/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Efficient Estimation of Word Representations in Vector Space">Efficient Estimation of Word Representations in Vector Space</span>
            <span class="post-date" title="2019-10-21 23:59:00">2019/10/21</span>
        </a>
        
        <a  class="CNN "
           href="/paper-fcn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="FCN">FCN</span>
            <span class="post-date" title="2019-12-24 23:59:00">2019/12/24</span>
        </a>
        
        <a  class="CNN "
           href="/paper-deeplab/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Deeplab">Deeplab</span>
            <span class="post-date" title="2019-12-23 23:59:00">2019/12/23</span>
        </a>
        
        <a  class="Paper "
           href="/paper-unet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="UNET">UNET</span>
            <span class="post-date" title="2019-10-18 23:59:00">2019/10/18</span>
        </a>
        
        <a  class="Paper "
           href="/paper-resnet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ResNet">ResNet</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="Paper "
           href="/paper-vgg/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="VGG Convolutional Neural Networks">VGG Convolutional Neural Networks</span>
            <span class="post-date" title="2020-01-15 23:59:00">2020/01/15</span>
        </a>
        
        <a  class="Math "
           href="/probability-theory-digital-characteristics/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="数字特征 Digital Characteristics">数字特征 Digital Characteristics</span>
            <span class="post-date" title="2015-04-18 23:59:00">2015/04/18</span>
        </a>
        
        <a  class="Math "
           href="/probability-theory-probability-theory/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="概率论">概率论</span>
            <span class="post-date" title="2018-03-23 23:59:00">2018/03/23</span>
        </a>
        
        <a  class="CNN "
           href="/python-numpy-nn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Numpy 构建神经网络">Numpy 构建神经网络</span>
            <span class="post-date" title="2018-04-06 01:20:12">2018/04/06</span>
        </a>
        
        <a  class="CNN "
           href="/python-numpy/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Numpy">Numpy</span>
            <span class="post-date" title="2018-01-06 01:20:12">2018/01/06</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-distri/"
           data-tag="distributed"
           data-author="" >
            <span class="post-title" title="Python之分布式并行">Python之分布式并行</span>
            <span class="post-date" title="2019-09-10 23:59:00">2019/09/10</span>
        </a>
        
        <a  class="CNN "
           href="/python-python-logging/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python logging 使用小贴士">Python logging 使用小贴士</span>
            <span class="post-date" title="2017-03-18 23:59:00">2017/03/18</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python 笔记">Python 笔记</span>
            <span class="post-date" title="2015-02-16 01:20:12">2015/02/16</span>
        </a>
        
        <a  class="编程语言 "
           href="/python-python-object/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python之面向对象">Python之面向对象</span>
            <span class="post-date" title="2019-09-04 23:59:00">2019/09/04</span>
        </a>
        
        <a  class="Pytorch "
           href="/pytorch-pytorch-faq/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Pytorch 使用常见问题查询手册">Pytorch 使用常见问题查询手册</span>
            <span class="post-date" title="2019-09-18 23:59:00">2019/09/18</span>
        </a>
        
        <a  class="Pytorch "
           href="/pytorch-pytorch-mutil-gpu/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Pytorch 多gpu并行训练">Pytorch 多gpu并行训练</span>
            <span class="post-date" title="2019-11-14 23:59:00">2019/11/14</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-cnn-optimization-algorithm" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">优化算法</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a  data-rel="CNN">CNN</a>
            
        </span>
        
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2020-04-17 14:02:12'>2020-03-23 23:59</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SGD-就是随机梯度下降"><span class="toc-text"><a href="#SGD-&#x5C31;&#x662F;&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;" class="headerlink" title="SGD &#x5C31;&#x662F;&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;"></a>SGD &#x5C31;&#x662F;&#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#momentum-动量加速-在SGD函数里指定-momentum-的值即可"><span class="toc-text"><a href="#momentum-&#x52A8;&#x91CF;&#x52A0;&#x901F;-&#x5728;SGD&#x51FD;&#x6570;&#x91CC;&#x6307;&#x5B9A;-momentum-&#x7684;&#x503C;&#x5373;&#x53EF;" class="headerlink" title="momentum &#x52A8;&#x91CF;&#x52A0;&#x901F;,&#x5728;SGD&#x51FD;&#x6570;&#x91CC;&#x6307;&#x5B9A; momentum &#x7684;&#x503C;&#x5373;&#x53EF;"></a>momentum &#x52A8;&#x91CF;&#x52A0;&#x901F;,&#x5728;SGD&#x51FD;&#x6570;&#x91CC;&#x6307;&#x5B9A; momentum &#x7684;&#x503C;&#x5373;&#x53EF;</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#指数加权移动平均"><span class="toc-text"><a href="#&#x6307;&#x6570;&#x52A0;&#x6743;&#x79FB;&#x52A8;&#x5E73;&#x5747;" class="headerlink" title="&#x6307;&#x6570;&#x52A0;&#x6743;&#x79FB;&#x52A8;&#x5E73;&#x5747;"></a>&#x6307;&#x6570;&#x52A0;&#x6743;&#x79FB;&#x52A8;&#x5E73;&#x5747;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#由指数加权移动平均理解动量法"><span class="toc-text"><a href="#&#x7531;&#x6307;&#x6570;&#x52A0;&#x6743;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x7406;&#x89E3;&#x52A8;&#x91CF;&#x6CD5;" class="headerlink" title="&#x7531;&#x6307;&#x6570;&#x52A0;&#x6743;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x7406;&#x89E3;&#x52A8;&#x91CF;&#x6CD5;"></a>&#x7531;&#x6307;&#x6570;&#x52A0;&#x6743;&#x79FB;&#x52A8;&#x5E73;&#x5747;&#x7406;&#x89E3;&#x52A8;&#x91CF;&#x6CD5;</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdaGrad-算法"><span class="toc-text"><a href="#AdaGrad-&#x7B97;&#x6CD5;" class="headerlink" title="AdaGrad &#x7B97;&#x6CD5;"></a>AdaGrad &#x7B97;&#x6CD5;</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RMSprop-指定参数-alpha"><span class="toc-text"><a href="#RMSprop-&#x6307;&#x5B9A;&#x53C2;&#x6570;-alpha" class="headerlink" title="RMSprop &#x6307;&#x5B9A;&#x53C2;&#x6570; alpha"></a>RMSprop &#x6307;&#x5B9A;&#x53C2;&#x6570; alpha</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdaDelta-算法"><span class="toc-text"><a href="#AdaDelta-&#x7B97;&#x6CD5;" class="headerlink" title="AdaDelta &#x7B97;&#x6CD5;"></a>AdaDelta &#x7B97;&#x6CD5;</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Adam-参数-betas-0-9-0-99"><span class="toc-text"><a href="#Adam-&#x53C2;&#x6570;-betas-0-9-0-99" class="headerlink" title="Adam &#x53C2;&#x6570; betas = (0.9, 0.99)"></a>Adam &#x53C2;&#x6570; betas = (0.9, 0.99)</span></a></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>训练一个复杂的深度学习模型可能需要数小时、数日，甚至数周时间，而优化算法的表现直接影响模型的训练效率；另一方面，理解各种优化算法的原理以及其中超参数的意义将有助于我们更有针对性地调参，从而使深度学习模型表现更好。<br>虽然优化为深度学习提供了最小化损失函数的方法，但本质上，优化与深度学习的目标是有区别的。<br>首先区分训练误差和泛化误差。 由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。 而深度学习的目标在于降低泛化误差。为了降低泛化误差，除了使用优化算法降低训练误差以外，还需要注意应对过拟合。</p>
<p>以下论述关注优化算法在最小化目标函数上的表现，而不关注模型的泛化误差。</p>
<h1 id="SGD-就是随机梯度下降"><a href="#SGD-就是随机梯度下降" class="headerlink" title="SGD 就是随机梯度下降"></a>SGD 就是随机梯度下降</h1><p>鞍点和局部最优的理解</p>
<p>随机梯度的计算公式：$X = X - \eta \nabla f(x)$<br>泰勒展开：$f(x + \epsilon) = f(x) + \frac {f’(x)}{1}\epsilon + o(\epsilon^2)$<br>令 $\epsilon = - \eta f’(x)$<br>得 $f(x - \eta f’(x)) = f(x) - \eta f’(x)^2 + o(\epsilon^2)$<br>$\eta$ 为正数，假设余项很小舍去，则 f(x) 会越来越小<br>前提条件，观察余项<br>所以学习率不能太大，一阶导数也不能太大</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scratch version</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p[:] -= hyperparams[<span class="string">'lr'</span>] * p.grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch version</span></span><br><span class="line">opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br></pre></td></tr></table></figure>
<h1 id="momentum-动量加速-在SGD函数里指定-momentum-的值即可"><a href="#momentum-动量加速-在SGD函数里指定-momentum-的值即可" class="headerlink" title="momentum 动量加速,在SGD函数里指定 momentum 的值即可"></a>momentum 动量加速,在SGD函数里指定 momentum 的值即可</h1><p>目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。<br>动量法的提出是为了解决梯度下降的上述问题。<br>$$<br>\begin{aligned}<br>\boldsymbol{v}_t &amp;\leftarrow \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,<br>\end{aligned}<br>$$</p>
<h3 id="指数加权移动平均"><a href="#指数加权移动平均" class="headerlink" title="指数加权移动平均"></a>指数加权移动平均</h3><p>$v_t = \gamma v_{t-1} + \eta_t grad$</p>
<p>为了从数学上理解动量法，让我们先解释一下指数加权移动平均（exponentially weighted moving average）。给定超参数$0 \leq \gamma &lt; 1$，当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合：</p>
<p>$$y_t = \gamma y_{t-1} + (1-\gamma) x_t.$$</p>
<p>$$<br>y^{(20)} = 0.05x^{(20)} + 0.95y^{(19)}<br>          = 0.05x^{(20)} + 0.05*0.95x^{(19)} + 0.95^2 y^{(18)}<br>          …<br>$$</p>
<p>$x^{\frac {1}{1-x}} \approx \frac {1}{e}$<br>$0.95^{20} \approx \frac {1}{e} = 0.3678$<br>代表对过去的 20 计算 EMA</p>
<p>我们可以对$y_t$展开：</p>
<p>$$<br>\begin{aligned}<br>y_t  &amp;= (1-\gamma) x_t + \gamma y_{t-1}\<br>         &amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\<br>         &amp;= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\<br>         &amp;\ldots<br>\end{aligned}<br>$$</p>
<p>令$n = 1/(1-\gamma)$，那么 $\left(1-1/n\right)^n = \gamma^{1/(1-\gamma)}$。因为</p>
<p>$$ \lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679,$$</p>
<p>所以当$\gamma \rightarrow 1$时，$\gamma^{1/(1-\gamma)}=\exp(-1)$，如$0.95^{20} \approx \exp(-1)$。如果把$\exp(-1)$当作一个比较小的数，我们可以在近似中忽略所有含$\gamma^{1/(1-\gamma)}$和比$\gamma^{1/(1-\gamma)}$更高阶的系数的项。例如，当$\gamma=0.95$时，</p>
<p>$$y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}.$$</p>
<p>因此，在实际中，我们常常将$y_t$看作是对最近$1/(1-\gamma)$个时间步的$x_t$值的加权平均。例如，当$\gamma = 0.95$时，$y_t$可以被看作对最近20个时间步的$x_t$值的加权平均；当$\gamma = 0.9$时，$y_t$可以看作是对最近10个时间步的$x_t$值的加权平均。而且，离当前时间步$t$越近的$x_t$值获得的权重越大（越接近1）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    v_b = nd.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = hyperparams[<span class="string">'momentum'</span>] * v + hyperparams[<span class="string">'lr'</span>] * p.grad</span><br><span class="line">        p[:] -= v</span><br></pre></td></tr></table></figure>
<h3 id="由指数加权移动平均理解动量法"><a href="#由指数加权移动平均理解动量法" class="headerlink" title="由指数加权移动平均理解动量法"></a>由指数加权移动平均理解动量法</h3><p>现在，我们对动量法的速度变量做变形：</p>
<p>$$\boldsymbol{v}_t \leftarrow \gamma \boldsymbol{v}_{t-1} + (1 - \gamma) \left(\frac{\eta_t}{1 - \gamma} \boldsymbol{g}_t\right). $$</p>
<p>由指数加权移动平均的形式可得，速度变量$\boldsymbol{v}_t$实际上对序列${\eta_{t-i}\boldsymbol{g}_{t-i} /(1-\gamma):i=0,\ldots,1/(1-\gamma)-1}$做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近$1/(1-\gamma)$个时间步的更新量做了指数加权移动平均后再除以$1-\gamma$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>
<h1 id="AdaGrad-算法"><a href="#AdaGrad-算法" class="headerlink" title="AdaGrad 算法"></a>AdaGrad 算法</h1><p>AdaGrad算法会使用一个小批量随机梯度$\boldsymbol{g}_t$按元素平方的累加变量$\boldsymbol{s}_t$。在时间步0，AdaGrad将$\boldsymbol{s}_0$中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\boldsymbol{g}_t$按元素平方后累加到变量$\boldsymbol{s}_t$：</p>
<p>$$\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,$$</p>
<p>其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<p>$$\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    s_b = nd.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] += p.grad.square()</span><br><span class="line">        p[:] -= hyperparams[<span class="string">'lr'</span>] * p.grad / (s + eps).sqrt()</span><br></pre></td></tr></table></figure>
<h1 id="RMSprop-指定参数-alpha"><a href="#RMSprop-指定参数-alpha" class="headerlink" title="RMSprop 指定参数 alpha"></a>RMSprop 指定参数 alpha</h1><p>我们在动量法里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \leq \gamma &lt; 1$，RMSProp算法在时间步$t&gt;0$计算</p>
<p>$$\boldsymbol{s}_t \leftarrow \gamma \boldsymbol{s}_{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t. $$</p>
<p>和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</p>
<p>$$\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t, $$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\boldsymbol{s}_t$是对平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\gamma)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p>
<p>照例，让我们先观察RMSProp算法对目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在<a href="adagrad.ipynb">“AdaGrad算法”</a>一节使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。<br>opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop_2d</span><span class="params">(x1, x2, s1, s2)</span>:</span></span><br><span class="line">    g1, g2, eps = <span class="number">0.2</span> * x1, <span class="number">4</span> * x2, <span class="number">1e-6</span></span><br><span class="line">    s1 = gamma * s1 + (<span class="number">1</span> - gamma) * g1 ** <span class="number">2</span></span><br><span class="line">    s2 = gamma * s2 + (<span class="number">1</span> - gamma) * g2 ** <span class="number">2</span></span><br><span class="line">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class="line">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class="line">    <span class="keyword">return</span> x1, x2, s1, s2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_2d</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.1</span> * x1 ** <span class="number">2</span> + <span class="number">2</span> * x2 ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">eta, gamma = <span class="number">0.4</span>, <span class="number">0.9</span></span><br><span class="line">d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure></p>
<h1 id="AdaDelta-算法"><a href="#AdaDelta-算法" class="headerlink" title="AdaDelta 算法"></a>AdaDelta 算法</h1><p>AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \leq \rho &lt; 1$（对应RMSProp算法中的$\gamma$），在时间步$t&gt;0$，同RMSProp算法一样计算</p>
<p>$$\boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t. $$</p>
<p>与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta\boldsymbol{x}<em>t$，其元素同样在时间步0时被初始化为0。我们使用$\Delta\boldsymbol{x}</em>{t-1}$来计算自变量的变化量：</p>
<p>$$ \boldsymbol{g}_t’ \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t, $$</p>
<p>其中$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：</p>
<p>$$\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}’_t. $$</p>
<p>最后，我们使用$\Delta\boldsymbol{x}_t$来记录自变量变化量$\boldsymbol{g}’_t$按元素平方的指数加权移动平均：</p>
<p>$$\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}’_t \odot \boldsymbol{g}’_t. $$</p>
<p>可以看到，如不考虑$\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\sqrt{\Delta\boldsymbol{x}_{t-1}}$来替代超参数$\eta$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span><span class="params">()</span>:</span></span><br><span class="line">    s_w, s_b = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>)), nd.zeros(<span class="number">1</span>)</span><br><span class="line">    delta_w, delta_b = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>)), nd.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    rho, eps = hyperparams[<span class="string">'rho'</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        s[:] = rho * s + (<span class="number">1</span> - rho) * p.grad.square()</span><br><span class="line">        g = ((delta + eps).sqrt() / (s + eps).sqrt()) * p.grad</span><br><span class="line">        p[:] -= g</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br></pre></td></tr></table></figure></p>
<h1 id="Adam-参数-betas-0-9-0-99"><a href="#Adam-参数-betas-0-9-0-99" class="headerlink" title="Adam 参数 betas = (0.9, 0.99)"></a>Adam 参数 betas = (0.9, 0.99)</h1><p>Adam 结合了两个想法来改善收敛性：每个参数更新可加快收敛速度；动量可避免卡在鞍点上。<br>Adam 算法使用了动量变量$\boldsymbol{v}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\boldsymbol{s}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \leq \beta_1 &lt; 1$（算法作者建议设为0.9），时间步$t$的动量变量$\boldsymbol{v}_t$即小批量随机梯度$\boldsymbol{g}_t$的指数加权移动平均：</p>
<p>$$\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t. $$</p>
<p>和RMSProp算法中一样，给定超参数$0 \leq \beta_2 &lt; 1$（算法作者建议设为0.999），<br>将小批量随机梯度按元素平方后的项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$做指数加权移动平均得到$\boldsymbol{s}_t$：</p>
<p>$$\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t. $$</p>
<p>由于我们将$\boldsymbol{v}_0$和$\boldsymbol{s}_0$中的元素都初始化为0，<br>在时间步$t$我们得到$\boldsymbol{v}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1 = 0.9$时，$\boldsymbol{v}_1 = 0.1\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\boldsymbol{v}_t$再除以$1 - \beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\boldsymbol{v}_t$和$\boldsymbol{s}_t$均作偏差修正：</p>
<p>$$\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}, $$</p>
<p>$$\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}. $$</p>
<p>接下来，Adam算法使用以上偏差修正后的变量$\hat{\boldsymbol{v}}_t$和$\hat{\boldsymbol{s}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：</p>
<p>$$\boldsymbol{g}_t’ \leftarrow \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon},$$</p>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\boldsymbol{g}_t’$迭代自变量：</p>
<p>$$\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t’. $$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scratch version</span></span><br><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w, v_b = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>)), nd.zeros(<span class="number">1</span>)</span><br><span class="line">    s_w, s_b = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>)), nd.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.square()</span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">'t'</span>])</span><br><span class="line">        p[:] -= hyperparams[<span class="string">'lr'</span>] * v_bias_corr / (s_bias_corr.sqrt() + eps)</span><br><span class="line">    hyperparams[<span class="string">'t'</span>] += <span class="number">1</span></span><br><span class="line"><span class="comment"># pytorch version</span></span><br><span class="line">opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>当使用小批量梯度下降时，为什么打乱数据很重要？<br>如果不打乱数据的顺序，那么假设我们训练一个神经网络分类器，且有两个类别：A和B，那么各个epoch中的所有小批量都会完全相同，这会导致收敛速度变慢，甚至导致神经网络对数据的顺序产生倾向性。</li>
<li>说明为何L2正则化可以解释为一种权重衰减。<br>$w = w -grad(C)(w) — 2cw = (1–2c)w — grad(C)(w)$<br>其中，1-2c &lt; 0，则 w 乘数因子 &lt; 1，所以 w 会变小。</li>
</ul>

      
       <hr><span style="font-style: italic;color: gray;"> 请多多指教。 </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>优化算法</p>
    
    <p><span class="copy-title">本文作者:</span><a  title="顺强">顺强</a></p>
    <p><span class="copy-title">发布时间:</span>2020-03-23, 23:59:00</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/cnn-optimization-algorithm/" title="优化算法">http://shun-qiang.github.io/cnn-optimization-algorithm/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: '84f77e333f2e54aee8e3',
            clientSecret: '9b89a9d1c0f55413c892dceedee0274ce40f34cf',
            repo: 'shun-qiang.github.io',
            owner: 'Shun-Qiang',
            admin: ['Shun-Qiang'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2020 SHUNQIANG</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>

<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#BFS','#Dynamic Programming','#Hard','#Binary Search','#Graph Theory','#quicksort','#Data Structure','#DFS','#distributed',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
        /* 渲染*/
        function HTMLDecode(text) {
            var temp = document.createElement("div");
            temp.innerHTML = text;
            var output = temp.innerText || temp.textContent;
            temp = null;
            return output;
        }
        if (window.mermaid){
            window.mermaid = null
        }
        $.getScript("//cdn.jsdelivr.net/npm/mermaid@8.4.2/dist/mermaid.min.js", function () {
            var mermaidOptions = JSON.parse(HTMLDecode("{&#34;theme&#34;:&#34;default&#34;,&#34;startOnLoad&#34;:true,&#34;flowchart&#34;:{&#34;useMaxWidth&#34;:true,&#34;htmlLabels&#34;:true}}"))
            if (window.mermaid) {
                mermaid.initialize(mermaidOptions)
                mermaid.contentLoaded()
            }
        })
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
