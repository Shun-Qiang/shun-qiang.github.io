<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <title>Pytorch 多gpu并行训练 | 寻梦乌托邦</title>
  <meta name="keywords" content="">
  <meta name="description" content="Pytorch 多gpu并行训练 | 寻梦乌托邦">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="正直 勇敢 追梦的人">
<meta property="og:type" content="website">
<meta property="og:title" content="分类">
<meta property="og:url" content="http://shun-qiang.github.io/categories/index.html">
<meta property="og:site_name" content="寻梦乌托邦">
<meta property="og:description" content="正直 勇敢 追梦的人">
<meta property="og:updated_time" content="2017-12-14T07:06:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="分类">
<meta name="twitter:description" content="正直 勇敢 追梦的人">


<link rel="icon" href="/img/avatar-sun.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1" ></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.0.1" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="false">
  <input class="theme_blog_path" value="">
</div>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar-sun.jpg" />
</a>
<div class="author">
    <span>顺强</span>
</div>

<div class="icon">
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(42)</small></div></li>
    
        
            
            <li><div data-rel="leetcode">leetcode<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="pytorch">pytorch<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="CNN">CNN<small>(25)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="CV">CV<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数据结构">数据结构<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="GIT">GIT<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="HEXO">HEXO<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="NUMPY">NUMPY<small>(3)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="PYTHON">PYTHON<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="标记语言">标记语言<small>(1)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="NLP">NLP<small>(1)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    </div>
    <div><a class="about  hasFriend  site_url"  href="/about">关于</a><a style="width: 50%"  class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="42">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode === 13){return false;}">
        <input id="local-search-input" class="search" type="text" placeholder="Search..." />
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none" />
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <div class="clearfix"></div>
</div>

    
    <nav id="title-list-nav">
        
        <a  class="leetcode "
           href="/104-leetcode/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="104. Maximum Depth of Binary Tree">104. Maximum Depth of Binary Tree</span>
            <span class="post-date" title="2019-12-28 23:59:00">2019/12/28</span>
        </a>
        
        <a  class="leetcode "
           href="/455-assign-cookies/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Assign Cookies 455 贪心算法">Assign Cookies 455 贪心算法</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="pytorch "
           href="/53-what-is-torchnn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="torch.nn 到底是什么？">torch.nn 到底是什么？</span>
            <span class="post-date" title="2019-12-28 23:59:00">2019/12/28</span>
        </a>
        
        <a  class=""
           href="/about/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="关于我">关于我</span>
            <span class="post-date" title="2019-11-01 09:36:58">2019/11/01</span>
        </a>
        
        <a  class="CNN "
           href="/activate-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="激活函数">激活函数</span>
            <span class="post-date" title="2019-02-02 23:59:00">2019/02/02</span>
        </a>
        
        <a  class="CNN "
           href="/bias-variance/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="偏差和方差（bias variance）过拟合 欠拟合（underfit overfit）">偏差和方差（bias variance）过拟合 欠拟合（underfit overfit）</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/bilinear-convtranspose/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="双线性插值和转置卷积">双线性插值和转置卷积</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/bn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Batch Normalization">Batch Normalization</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/convolution/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="卷积">卷积</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/cnn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="CNN">CNN</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="CV "
           href="/cv-task/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="CV Tasks">CV Tasks</span>
            <span class="post-date" title="2019-12-01 23:59:00">2019/12/01</span>
        </a>
        
        <a  class="CNN "
           href="/data-augmentation/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="数据增强 Data Augmentation">数据增强 Data Augmentation</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="数据结构 "
           href="/data-structure/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Tree">Tree</span>
            <span class="post-date" title="2015-04-18 23:59:00">2015/04/18</span>
        </a>
        
        <a  class="CNN "
           href="/deep-learning-start/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="深度学习">深度学习</span>
            <span class="post-date" title="2019-02-18 23:59:00">2019/02/18</span>
        </a>
        
        <a  class="CNN "
           href="/deeplab/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Deeplab">Deeplab</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/eval/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="评估标准">评估标准</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="CNN "
           href="/fcn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="FCN">FCN</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/feeling/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ResNet">ResNet</span>
            <span class="post-date" title="2019-07-10 01:20:12">2019/07/10</span>
        </a>
        
        <a  class="GIT "
           href="/git-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Git 笔记">Git 笔记</span>
            <span class="post-date" title="2015-02-16 01:20:12">2015/02/16</span>
        </a>
        
        <a  class="HEXO "
           href="/hexo-troubleshoot/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Hexo troubleshooting">Hexo troubleshooting</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/image-caption/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="image-caption">image-caption</span>
            <span class="post-date" title="2020-02-03 23:59:00">2020/02/03</span>
        </a>
        
        <a  class="CNN "
           href="/inception/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Inception">Inception</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/lane-summary/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="项目总结">项目总结</span>
            <span class="post-date" title="2020-03-23 23:59:00">2020/03/23</span>
        </a>
        
        <a  class="CNN "
           href="/learning-rate/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="深度学习超参数">深度学习超参数</span>
            <span class="post-date" title="2019-02-18 23:59:00">2019/02/18</span>
        </a>
        
        <a  class="NUMPY "
           href="/linear-regression/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="线性回归 Linear Regression">线性回归 Linear Regression</span>
            <span class="post-date" title="2019-12-06 01:20:12">2019/12/06</span>
        </a>
        
        <a  class="PYTHON "
           href="/linux-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Linux 笔记">Linux 笔记</span>
            <span class="post-date" title="2015-01-06 01:20:12">2015/01/06</span>
        </a>
        
        <a  class="CNN "
           href="/loss-function/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="LOSS函数">LOSS函数</span>
            <span class="post-date" title="2019-03-05 23:59:00">2019/03/05</span>
        </a>
        
        <a  class="标记语言 "
           href="/markdown/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MARKDOWN 小知识">MARKDOWN 小知识</span>
            <span class="post-date" title="2017-12-18 23:59:00">2017/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/mobilenet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MobileNet">MobileNet</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="NUMPY "
           href="/numpy-nn/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Numpy 构建神经网络">Numpy 构建神经网络</span>
            <span class="post-date" title="2018-01-06 01:20:12">2018/01/06</span>
        </a>
        
        <a  class="NUMPY "
           href="/numpy/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Numpy">Numpy</span>
            <span class="post-date" title="2018-01-06 01:20:12">2018/01/06</span>
        </a>
        
        <a  class="CNN "
           href="/pooling/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="池化层">池化层</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/python-logging/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python logging 使用小贴士">Python logging 使用小贴士</span>
            <span class="post-date" title="2017-03-18 23:59:00">2017/03/18</span>
        </a>
        
        <a  class="PYTHON "
           href="/python-note/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Python 笔记">Python 笔记</span>
            <span class="post-date" title="2015-01-06 01:20:12">2015/01/06</span>
        </a>
        
        <a  class="pytorch "
           href="/pytorch-faq/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Pytorch 使用常见问题查询手册">Pytorch 使用常见问题查询手册</span>
            <span class="post-date" title="2019-12-28 23:59:00">2019/12/28</span>
        </a>
        
        <a  class="pytorch "
           href="/pytorch-mutil-gpu/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Pytorch 多gpu并行训练">Pytorch 多gpu并行训练</span>
            <span class="post-date" title="2019-12-29 23:59:00">2019/12/29</span>
        </a>
        
        <a  class="CNN "
           href="/receptive-field/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="感受野">感受野</span>
            <span class="post-date" title="2019-01-18 23:59:00">2019/01/18</span>
        </a>
        
        <a  class="CNN "
           href="/resnet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ResNet">ResNet</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/rethink-relu-to/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Rethinking ReLU to Train Better CNNs">Rethinking ReLU to Train Better CNNs</span>
            <span class="post-date" title="2015-02-01 23:59:00">2015/02/01</span>
        </a>
        
        <a  class="CNN "
           href="/unet/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="UNET">UNET</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="NLP "
           href="/word-vector/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Efficient Estimation of Word Representations in Vector Space">Efficient Estimation of Word Representations in Vector Space</span>
            <span class="post-date" title="2019-12-18 23:59:00">2019/12/18</span>
        </a>
        
        <a  class="CNN "
           href="/yolov2/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="YOLO9000 Better, Faster, Stronger">YOLO9000 Better, Faster, Stronger</span>
            <span class="post-date" title="2020-03-18 23:59:00">2020/03/18</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-pytorch-mutil-gpu" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">Pytorch 多gpu并行训练</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a  data-rel="pytorch">pytorch</a>
            
        </span>
        
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title='更新时间: 2020-03-24 17:08:02'>2019-12-29 23:59</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#单机多卡并行训练"><span class="toc-text"><a href="#&#x5355;&#x673A;&#x591A;&#x5361;&#x5E76;&#x884C;&#x8BAD;&#x7EC3;" class="headerlink" title="&#x5355;&#x673A;&#x591A;&#x5361;&#x5E76;&#x884C;&#x8BAD;&#x7EC3;"></a>&#x5355;&#x673A;&#x591A;&#x5361;&#x5E76;&#x884C;&#x8BAD;&#x7EC3;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多机多-gpu-训练"><span class="toc-text"><a href="#&#x591A;&#x673A;&#x591A;-gpu-&#x8BAD;&#x7EC3;" class="headerlink" title="&#x591A;&#x673A;&#x591A; gpu &#x8BAD;&#x7EC3;"></a>&#x591A;&#x673A;&#x591A; gpu &#x8BAD;&#x7EC3;</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FAQ"><span class="toc-text"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="单机多卡并行训练"><a href="#单机多卡并行训练" class="headerlink" title="单机多卡并行训练"></a>单机多卡并行训练</h2><ol>
<li><p>torch.nn.DataParallel<br>如果不设定好要使用的 device_ids 的话, 程序会自动找到这个机器上面可以用的所有的显卡, 然后用于训练. 但是因为我们前面使用 os.environ[‘CUDA_VISIBLE_DEVICES’] 限定了这个程序可以使用的显卡, 所以这个地方程序如果自己获取的话, 获取到的其实就是我们上面设定的那几个显卡 </p>
<pre><code>device_ids = list(range(torch.cuda.device_count())) 。
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataParallel</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, module, device_ids=None, output_device=None, dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(DataParallel, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">            self.module = module</span><br><span class="line">            self.device_ids = []</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> device_ids <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            device_ids = list(range(torch.cuda.device_count()))</span><br><span class="line">        <span class="keyword">if</span> output_device <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            output_device = device_ids[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>如何平衡 DataParallel 带来的显存使用不平衡的问题</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">my_net = MyNet()</span><br><span class="line">my_net = BalancedDataParallel(gpu0_bsz // acc_grad, my_net, dim=<span class="number">0</span>).cuda()</span><br><span class="line"><span class="comment"># 第一个参数是第一个GPU要分配多大的batch_size, 但是要注意, 如果你使用了梯度累积, 那么这里传入的是每次进行运算的实际batch_size大小. 举个例子, 比如你在3个GPU上面跑代码, 但是一个GPU最大只能跑3条数据, 但是因为0号GPU还要做一些数据的整合操作, 于是0号GPU只能跑2条数据, 这样一算, 你可以跑的大小是2+3+3=8, 于是你可以设置下面的这样的参数:</span></span><br><span class="line">batch_szie = <span class="number">8</span></span><br><span class="line">gpu0_bsz = <span class="number">2</span></span><br><span class="line">acc_grad = <span class="number">1</span></span><br><span class="line">my_net = MyNet()</span><br><span class="line">my_net = BalancedDataParallel(gpu0_bsz // acc_grad, my_net, dim=<span class="number">0</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch size是16的怎么办呢, 那就是4+6+6=16了, 这样设置累积梯度为2就行了:</span></span><br><span class="line">batch_szie = <span class="number">16</span></span><br><span class="line">gpu0_bsz = <span class="number">4</span></span><br><span class="line">acc_grad = <span class="number">2</span></span><br><span class="line">my_net = MyNet()</span><br><span class="line">my_net = BalancedDataParallel(gpu0_bsz // acc_grad, my_net, dim=<span class="number">0</span>).cuda()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DataParallel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel._functions <span class="keyword">import</span> Scatter</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel.parallel_apply <span class="keyword">import</span> parallel_apply</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter</span><span class="params">(inputs, target_gpus, chunk_sizes, dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">    Slices tensors into approximately equal chunks and</span></span><br><span class="line"><span class="string">    distributes them across given GPUs. Duplicates</span></span><br><span class="line"><span class="string">    references to objects that are not tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scatter_map</span><span class="params">(obj)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(obj, torch.Tensor):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">return</span> Scatter.apply(target_gpus, chunk_sizes, dim, obj)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">'obj'</span>, obj.size())</span><br><span class="line">                print(<span class="string">'dim'</span>, dim)</span><br><span class="line">                print(<span class="string">'chunk_sizes'</span>, chunk_sizes)</span><br><span class="line">                quit()</span><br><span class="line">        <span class="keyword">if</span> isinstance(obj, tuple) <span class="keyword">and</span> len(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> list(zip(*map(scatter_map, obj)))</span><br><span class="line">        <span class="keyword">if</span> isinstance(obj, list) <span class="keyword">and</span> len(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> list(map(list, zip(*map(scatter_map, obj))))</span><br><span class="line">        <span class="keyword">if</span> isinstance(obj, dict) <span class="keyword">and</span> len(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> list(map(type(obj), zip(*map(scatter_map, obj.items()))))</span><br><span class="line">        <span class="keyword">return</span> [obj <span class="keyword">for</span> targets <span class="keyword">in</span> target_gpus]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># After scatter_map is called, a scatter_map cell will exist. This cell</span></span><br><span class="line">    <span class="comment"># has a reference to the actual function scatter_map, which has references</span></span><br><span class="line">    <span class="comment"># to a closure that has a reference to the scatter_map cell (because the</span></span><br><span class="line">    <span class="comment"># fn is recursive). To avoid this reference cycle, we set the function to</span></span><br><span class="line">    <span class="comment"># None, clearing the cell</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> scatter_map(inputs)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        scatter_map = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter_kwargs</span><span class="params">(inputs, kwargs, target_gpus, chunk_sizes, dim=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">r"""Scatter with support for kwargs dictionary"""</span></span><br><span class="line">    inputs = scatter(inputs, target_gpus, chunk_sizes, dim) <span class="keyword">if</span> inputs <span class="keyword">else</span> []</span><br><span class="line">    kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) <span class="keyword">if</span> kwargs <span class="keyword">else</span> []</span><br><span class="line">    <span class="keyword">if</span> len(inputs) &lt; len(kwargs):</span><br><span class="line">        inputs.extend([() <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(kwargs) - len(inputs))])</span><br><span class="line">    <span class="keyword">elif</span> len(kwargs) &lt; len(inputs):</span><br><span class="line">        kwargs.extend([&#123;&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(inputs) - len(kwargs))])</span><br><span class="line">    inputs = tuple(inputs)</span><br><span class="line">    kwargs = tuple(kwargs)</span><br><span class="line">    <span class="keyword">return</span> inputs, kwargs</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BalancedDataParallel</span><span class="params">(DataParallel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, gpu0_bsz, *args, **kwargs)</span>:</span></span><br><span class="line">        self.gpu0_bsz = gpu0_bsz</span><br><span class="line">        super().__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, *inputs, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.device_ids:</span><br><span class="line">            <span class="keyword">return</span> self.module(*inputs, **kwargs)</span><br><span class="line">        <span class="keyword">if</span> self.gpu0_bsz == <span class="number">0</span>:</span><br><span class="line">            device_ids = self.device_ids[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            device_ids = self.device_ids</span><br><span class="line">        inputs, kwargs = self.scatter(inputs, kwargs, device_ids)</span><br><span class="line">        <span class="keyword">if</span> len(self.device_ids) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> self.module(*inputs[<span class="number">0</span>], **kwargs[<span class="number">0</span>])</span><br><span class="line">        replicas = self.replicate(self.module, self.device_ids)</span><br><span class="line">        <span class="keyword">if</span> self.gpu0_bsz == <span class="number">0</span>:</span><br><span class="line">            replicas = replicas[<span class="number">1</span>:]</span><br><span class="line">        outputs = self.parallel_apply(replicas, device_ids, inputs, kwargs)</span><br><span class="line">        <span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parallel_apply</span><span class="params">(self, replicas, device_ids, inputs, kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> parallel_apply(replicas, inputs, kwargs, device_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scatter</span><span class="params">(self, inputs, kwargs, device_ids)</span>:</span></span><br><span class="line">        bsz = inputs[<span class="number">0</span>].size(self.dim)</span><br><span class="line">        num_dev = len(self.device_ids)</span><br><span class="line">        gpu0_bsz = self.gpu0_bsz</span><br><span class="line">        bsz_unit = (bsz - gpu0_bsz) // (num_dev - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> gpu0_bsz &lt; bsz_unit:</span><br><span class="line">            chunk_sizes = [gpu0_bsz] + [bsz_unit] * (num_dev - <span class="number">1</span>)</span><br><span class="line">            delta = bsz - sum(chunk_sizes)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(delta):</span><br><span class="line">                chunk_sizes[i + <span class="number">1</span>] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> gpu0_bsz == <span class="number">0</span>:</span><br><span class="line">                chunk_sizes = chunk_sizes[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> super().scatter(inputs, kwargs, device_ids)</span><br><span class="line">        <span class="keyword">return</span> scatter_kwargs(inputs, kwargs, device_ids, chunk_sizes, dim=self.dim)</span><br></pre></td></tr></table></figure>
<ol>
<li>torch.nn.parallel.DistributedDataParallel<br><a href="https://pytorch.org/docs/stable/nn.html#distributeddataparallel" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#distributeddataparallel</a><br>先将model加载到GPU, 然后才能使用DistributedDataParallel进行分发, 之后的使用和DataParallel就基本一样了<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = model.cuda()</span><br><span class="line">model = nn.parallel.DistributedDataParallel(model)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="多机多-gpu-训练"><a href="#多机多-gpu-训练" class="headerlink" title="多机多 gpu 训练"></a>多机多 gpu 训练</h2><p>为了防止多机网络带宽限制数据传输速度，尽量使用单机多卡</p>
<ol>
<li><p>初始化<br>使用torch.distributed.init_process_group()进行初始化. torch.distributed.init_process_group()包含四个常用的参数：<br>backend: 后端, 实际上是多个机器之间交换数据的协议<br>init_method: 机器之间交换数据, 需要指定一个主节点, 而这个参数就是指定主节点的<br>world_size: 介绍都是说是进程, 实际就是机器的个数, 例如两台机器一起训练的话, world_size就设置为2<br>rank: 区分主节点和从节点的, 主节点为0, 剩余的为了1-(N-1), N为要使用的机器的数量, 也就是world_size</p>
</li>
<li><p>数据的处理</p>
</li>
<li><p>模型的处理</p>
</li>
<li><p>模型的保存</p>
</li>
</ol>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ol>
<li>RuntimeError: CUDA error: invalid device ordinal<br>使用 os.environ[‘CUDA_VISIBLE_DEVICES’] 限定了这个程序可以使用的显卡, 所以这个地方程序如果自己获取的话, 获取到的其实就是我们上面设定的那几个显卡.<br>我没有进行深入得到考究, 但是我感觉使用 os.environ[‘CUDA_VISIBLE_DEVICES’] 对可以使用的显卡进行限定之后, 显卡的实际编号和程序看到的编号应该是不一样的, 例如上面我们设定的是 os.environ[‘CUDA_VISIBLE_DEVICES’]=”0,2”, 但是程序看到的显卡编号应该被改成了’0,1’, 也就是说程序所使用的显卡编号实际上是经过了一次映射之后才会映射到真正的显卡编号上面的, 例如这里的程序看到的 1 对应实际的 2</li>
</ol>
<p>参考链接：<br>OPTIONAL: DATA PARALLELISM<br><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</a><br><a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py" target="_blank" rel="noopener">https://github.com/pytorch/examples/blob/master/imagenet/main.py</a><br>Getting Started with Distributed Data Parallel<br><a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a><br><a href="https://discuss.pytorch.org/t/cuda-visible-devices-make-gpu-disappear/21439/10" target="_blank" rel="noopener">https://discuss.pytorch.org/t/cuda-visible-devices-make-gpu-disappear/21439/10</a><br><a href="https://discuss.pytorch.org/t/cuda-visible-device-is-of-no-use/10018" target="_blank" rel="noopener">https://discuss.pytorch.org/t/cuda-visible-device-is-of-no-use/10018</a><br><a href="https://devblogs.nvidia.com/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/" target="_blank" rel="noopener">https://devblogs.nvidia.com/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/</a><br><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener">https://github.com/kimiyoung/transformer-xl</a><br><a href="https://github.com/Link-Li/Balanced-DataParallel" target="_blank" rel="noopener">https://github.com/Link-Li/Balanced-DataParallel</a></p>

      
       <hr><span style="font-style: italic;color: gray;"> 请多多指教。 </span>
    </div>
</article>


<p>
    <a  class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>Pytorch 多gpu并行训练</p>
    
    <p><span class="copy-title">本文作者:</span><a  title="顺强">顺强</a></p>
    <p><span class="copy-title">发布时间:</span>2019-12-29, 23:59:00</p>
    <p><span class="copy-title">最后更新:</span>2020-03-24, 17:08:02</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/pytorch-mutil-gpu/" title="Pytorch 多gpu并行训练">http://shun-qiang.github.io/pytorch-mutil-gpu/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: '84f77e333f2e54aee8e3',
            clientSecret: '9b89a9d1c0f55413c892dceedee0274ce40f34cf',
            repo: 'shun-qiang.github.io',
            owner: 'Shun-Qiang',
            admin: ['Shun-Qiang'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2020 SHUNQIANG</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class="" id="rocket" ></a>

    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close"  onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
<script src="/js/jquery.pjax.js?v=1.0.1" ></script>

<script src="/js/script.js?v=1.0.1" ></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': [],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
